{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ASSIGNMENT - 4  \n",
    "> 1.Download the dataset  \n",
    "> 2.Import required library  \n",
    "> In \\[4\\]:\n",
    ">\n",
    "> **import** pandas **as** pd  \n",
    "> **import** numpy **as** np  \n",
    "> **import** matplotlib.pyplot **as** plt  \n",
    "> **import** seaborn **as** sns  \n",
    "> **from** sklearn.model_selection **import** train_test_split  \n",
    "> **from** sklearn.preprocessing **import** LabelEncoder  \n",
    "> **from** keras.models **import** Model  \n",
    "> **from** keras.layers **import** LSTM, Activation, Dense, Dropout,\n",
    "> Input, Embedding **from** keras.optimizers **import** RMSprop  \n",
    "> **from** keras.preprocessing.text **import** Tokenizer  \n",
    "> **from** keras.preprocessing **import** sequence  \n",
    "> **from** keras.utils **import** pad_sequences  \n",
    "> **from** keras.utils **import** to_categorical  \n",
    "> **from** keras.callbacks **import** EarlyStopping\n",
    ">\n",
    "> 3.Read Dataset and do preprocessing  \n",
    "> In \\[6\\]:\n",
    ">\n",
    "> data**=**pd**.**read_csv('/content/spam.csv',encoding**=**'latin')\n",
    ">\n",
    "> In \\[7\\]: df **=**\n",
    "> pd**.**read_csv('/content/spam.csv',delimiter**=**',',encoding**=**'latin-1')\n",
    "> df**.**head()\n",
    "\n",
    "Out\\[7\\]:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr class=\"header\">\n",
    "<th><strong>0</strong></th>\n",
    "<th><strong>v1</strong></th>\n",
    "<th><strong>v2</strong></th>\n",
    "<th><strong>Unnamed: 2</strong></th>\n",
    "<th><strong>Unnamed: 3</strong></th>\n",
    "<th><blockquote>\n",
    "<p><strong>Unnamed: 4</strong></p>\n",
    "</blockquote></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"odd\">\n",
    "<td></td>\n",
    "<td>ham</td>\n",
    "<td><blockquote>\n",
    "<p>Go until jurong point, crazy.. Available only ...</p>\n",
    "</blockquote></td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td><strong>1</strong></td>\n",
    "<td>ham</td>\n",
    "<td>Ok lar... Joking wif u oni...</td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td><strong>2</strong></td>\n",
    "<td>spam</td>\n",
    "<td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td><strong>3</strong></td>\n",
    "<td>ham</td>\n",
    "<td><blockquote>\n",
    "<p>U dun say so early hor... U c already then say...</p>\n",
    "</blockquote></td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "</tr>\n",
    "<tr class=\"odd\">\n",
    "<td><strong>4</strong></td>\n",
    "<td>ham</td>\n",
    "<td><blockquote>\n",
    "<p>Nah I don't think he goes to usf, he lives aro...</p>\n",
    "</blockquote></td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "<td>NaN</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "> In \\[8\\]: df**.**drop(\\['Unnamed: 2', 'Unnamed: 3', 'Unnamed:\n",
    "> 4'\\],axis**=**1,inplace**=True**) df**.**info()\n",
    ">\n",
    "> \\<class 'pandas.core.frame.DataFrame'\\>  \n",
    "> RangeIndex: 5572 entries, 0 to 5571\n",
    ">\n",
    "> Data columns (total 2 columns):  \n",
    "> \\# Column Non-Null Count Dtype  \n",
    "> --- ------ -------------- -----  \n",
    "> 0 v1 5572 non-null object  \n",
    "> 1 v2 5572 non-null object  \n",
    "> dtypes: object(2)  \n",
    "> memory usage: 87.2+ KB\n",
    ">\n",
    "> In \\[9\\]: *\\# Count of Spam and Ham values*  \n",
    "> df**.**groupby(\\['v1'\\])**.**size()\n",
    ">\n",
    "> Out\\[9\\]: v1  \n",
    "> ham 4825  \n",
    "> spam 747  \n",
    "> dtype: int64\n",
    ">\n",
    "> In \\[10\\]: *\\# Label Encoding target column*  \n",
    "> X **=** df**.**v2  \n",
    "> Y **=** df**.**v1  \n",
    "> le **=** LabelEncoder()  \n",
    "> Y **=** le**.**fit_transform(Y)  \n",
    "> Y **=** Y**.**reshape(**-**1,1)\n",
    ">\n",
    "> In \\[11\\]: *\\# Test and train split*  \n",
    "> X_train,X_test,Y_train,Y_test **=**\n",
    "> train_test_split(X,Y,test_size**=**0.15)\n",
    ">\n",
    "> In \\[12\\]: *\\# Tokenisation function*  \n",
    "> max_words **=** 1000  \n",
    "> max_len **=** 150  \n",
    "> tok **=** Tokenizer(num_words**=**max_words)  \n",
    "> tok**.**fit_on_texts(X_train)  \n",
    "> sequences **=** tok**.**texts_to_sequences(X_train)  \n",
    "> sequences_matrix **=** pad_sequences(sequences,maxlen**=**max_len)\n",
    ">\n",
    "> 4.Create Model and 5. Add Layers (LSTM, Dense-(Hidden Layers), Output)\n",
    ">\n",
    "> In \\[13\\]: *\\# Creating LSTM model*  \n",
    "> inputs **=** Input(name**=**'inputs',shape**=**\\[max_len\\])  \n",
    "> layer **=** Embedding(max_words,50,input_length**=**max_len)(inputs)  \n",
    "> layer **=** LSTM(64)(layer)  \n",
    "> layer **=** Dense(256,name**=**'FC1')(layer)  \n",
    "> layer **=** Activation('relu')(layer)  \n",
    "> layer **=** Dropout(0.5)(layer)  \n",
    "> layer **=** Dense(1,name**=**'out_layer')(layer)  \n",
    "> layer **=** Activation('sigmoid')(layer)  \n",
    "> model **=** Model(inputs**=**inputs,outputs**=**layer)\n",
    ">\n",
    "> 6.Compile the model & 7.Fit the Model  \n",
    "> In \\[14\\]:\n",
    ">\n",
    "> model**.**summary()\n",
    ">\n",
    "> model**.**compile(loss**=**'binary_crossentropy',optimizer**=**RMSprop(),metrics**=**\\['accura\n",
    "> cy'\\])  \n",
    "> model**.**fit(sequences_matrix,Y_train,batch_size**=**128,epochs**=**10,  \n",
    "> validation_split**=**0.2)\n",
    ">\n",
    "> Model: \"model\"  \n",
    "> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_  \n",
    "> Layer (type) Output Shape Param \\#  \n",
    "> =================================================================  \n",
    "> inputs (InputLayer) \\[(None, 150)\\] 0  \n",
    "> embedding (Embedding) (None, 150, 50) 50000  \n",
    "> lstm (LSTM) (None, 64) 29440  \n",
    "> FC1 (Dense) (None, 256) 16640  \n",
    "> activation (Activation) (None, 256) 0  \n",
    "> dropout (Dropout) (None, 256) 0  \n",
    "> out_layer (Dense) (None, 1) 257  \n",
    "> activation_1 (Activation) (None, 1) 0  \n",
    "> =================================================================  \n",
    "> Total params: 96,337  \n",
    "> Trainable params: 96,337  \n",
    "> Non-trainable params: 0  \n",
    "> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_  \n",
    "> Epoch 1/10  \n",
    "> 30/30 \\[==============================\\] - 12s 311ms/step - loss:\n",
    "> 0.3269 - accu racy: 0.8746 - val_loss: 0.1554 - val_accuracy: 0.9778  \n",
    "> Epoch 2/10  \n",
    "> 30/30 \\[==============================\\] - 12s 380ms/step - loss:\n",
    "> 0.0819 - accu racy: 0.9794 - val_loss: 0.0461 - val_accuracy: 0.9831  \n",
    "> Epoch 3/10  \n",
    "> 30/30 \\[==============================\\] - 8s 259ms/step - loss:\n",
    "> 0.0474 - accur acy: 0.9873 - val_loss: 0.0343 - val_accuracy: 0.9905  \n",
    "> Epoch 4/10  \n",
    "> 30/30 \\[==============================\\] - 8s 261ms/step - loss:\n",
    "> 0.0370 - accur acy: 0.9894 - val_loss: 0.0447 - val_accuracy: 0.9895  \n",
    "> Epoch 5/10  \n",
    "> 30/30 \\[==============================\\] - 9s 310ms/step - loss:\n",
    "> 0.0276 - accur acy: 0.9918 - val_loss: 0.0340 - val_accuracy: 0.9905  \n",
    "> Epoch 6/10  \n",
    "> 30/30 \\[==============================\\] - 8s 260ms/step - loss:\n",
    "> 0.0224 - accur acy: 0.9929 - val_loss: 0.0402 - val_accuracy: 0.9905  \n",
    "> Epoch 7/10  \n",
    "> 30/30 \\[==============================\\] - 8s 260ms/step - loss:\n",
    "> 0.0159 - accur acy: 0.9958 - val_loss: 0.0442 - val_accuracy: 0.9916  \n",
    "> Epoch 8/10\n",
    ">\n",
    "> 30/30 \\[==============================\\] - 8s 257ms/step - loss:\n",
    "> 0.0141 - accur acy: 0.9960 - val_loss: 0.0433 - val_accuracy: 0.9905  \n",
    "> Epoch 9/10  \n",
    "> 30/30 \\[==============================\\] - 8s 261ms/step - loss:\n",
    "> 0.0108 - accur acy: 0.9974 - val_loss: 0.0952 - val_accuracy: 0.9736  \n",
    "> Epoch 10/10  \n",
    "> 30/30 \\[==============================\\] - 8s 260ms/step - loss:\n",
    "> 0.0089 - accur acy: 0.9979 - val_loss: 0.0607 - val_accuracy: 0.9884\n",
    "\n",
    "Out\\[14\\]:\n",
    "\n",
    "> \\<keras.callbacks.History at 0x7f823de6acd0\\> 8.Save the Model\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "> model**.**save('sms_classifier.h5')\n",
    ">\n",
    "> 9.Test the model  \n",
    "> In \\[16\\]:\n",
    ">\n",
    "> test_sequences **=** tok**.**texts_to_sequences(X_test)  \n",
    "> test_sequences_matrix **=**\n",
    "> pad_sequences(test_sequences,maxlen**=**max_len)\n",
    "\n",
    "In \\[17\\]:\n",
    "\n",
    "> accr **=** model**.**evaluate(test_sequences_matrix,Y_test)\n",
    ">\n",
    "> 27/27 \\[==============================\\] - 1s 23ms/step - loss: 0.1038\n",
    "> - accura cy: 0.9856\n",
    ">\n",
    "> In \\[18\\]: print('Test set\\\\n Loss: {:0.3f}\\\\n Accuracy:  \n",
    "> {:0.3f}'**.**format(accr\\[0\\],accr\\[1\\]))\n",
    ">\n",
    "> Test set  \n",
    "> Loss: 0.104  \n",
    "> Accuracy: 0.986"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
